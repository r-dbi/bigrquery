[{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@posit.co. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to bigrquery","title":"Contributing to bigrquery","text":"outlines propose change bigrquery. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to bigrquery","text":"Small typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. YES: edit roxygen comment .R file R/. : edit .Rd file man/.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CONTRIBUTING.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"Contributing to bigrquery","text":"make substantial pull request, always file issue make sure someone team agrees ’s problem. ’ve found bug, create associated issue illustrate bug minimal reprex.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"","what":"Pull request process","title":"Contributing to bigrquery","text":"recommend create Git branch pull request (PR). Look Travis AppVeyor build status making changes. README contain badges continuous integration services used package. New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat. Contributions test cases included easier accept. user-facing changes, add bullet top NEWS.md current development version header describing changes made followed GitHub username, links relevant issue(s)/PR(s).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CONTRIBUTING.html","id":"test-project-and-bucket","dir":"","previous_headings":"","what":"Test project and bucket","title":"Contributing to bigrquery","text":"See internal help bq_test_project() information setting test project test bucket.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CONTRIBUTING.html","id":"testing-token","dir":"","previous_headings":"","what":"Testing token","title":"Contributing to bigrquery","text":"overall approach managing service account token used tests described gargle article Managing tokens securely.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to bigrquery","text":"Please note bigrquery project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/CONTRIBUTING.html","id":"see-tidyverse-development-contributing-guide","dir":"","previous_headings":"","what":"See tidyverse development contributing guide","title":"Contributing to bigrquery","text":"details.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 bigrquery authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with bigrquery","title":"Getting help with bigrquery","text":"Thanks using bigrquery. filing issue, places explore pieces put together make process smooth possible. Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty insane ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site. Armed reprex, next step figure ask. ’s question: start community.rstudio.com, /StackOverflow. people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed. right place, need file issue, please review “File issues” paragraph tidyverse contributing guidelines. Thanks help!","code":""},{"path":"https://bigrquery.r-dbi.org/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hadley Wickham. Author, maintainer. Jennifer Bryan. Author. . Copyright holder, funder.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wickham H, Bryan J (2025). bigrquery: Interface Google's 'BigQuery' 'API'. R package version 1.5.1.9000, https://bigrquery.r-dbi.org.","code":"@Manual{,   title = {bigrquery: An Interface to Google's 'BigQuery' 'API'},   author = {Hadley Wickham and Jennifer Bryan},   year = {2025},   note = {R package version 1.5.1.9000},   url = {https://bigrquery.r-dbi.org}, }"},{"path":"https://bigrquery.r-dbi.org/dev/index.html","id":"bigrquery","dir":"","previous_headings":"","what":"An Interface to Googles BigQuery API'","title":"An Interface to Googles BigQuery API'","text":"bigrquery package makes easy work data stored Google BigQuery allowing query BigQuery tables retrieve metadata projects, datasets, tables, jobs. bigrquery package provides three levels abstraction top BigQuery: low-level API provides thin wrappers underlying REST API. low-level functions start bq_, mostly form bq_noun_verb(). level abstraction appropriate ’re familiar REST API want something supported higher-level APIs. DBI interface wraps low-level API makes working BigQuery like working database system. convenient layer want execute SQL queries BigQuery upload smaller amounts (.e. <100 MB) data. dplyr interface lets treat BigQuery tables -memory data frames. convenient layer don’t want write SQL, instead want dbplyr write .","code":""},{"path":"https://bigrquery.r-dbi.org/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An Interface to Googles BigQuery API'","text":"current bigrquery release can installed CRAN: newest development release can installed GitHub:","code":"install.packages(\"bigrquery\") #install.packages(\"pak\") pak::pak(\"r-dbi/bigrquery\")"},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/index.html","id":"low-level-api","dir":"","previous_headings":"Usage","what":"Low-level API","title":"An Interface to Googles BigQuery API'","text":"","code":"library(bigrquery) billing <- bq_test_project() # replace this with your project ID  sql <- \"SELECT year, month, day, weight_pounds FROM `publicdata.samples.natality`\"  tb <- bq_project_query(billing, sql) bq_table_download(tb, n_max = 10) #> # A tibble: 10 × 4 #>     year month   day weight_pounds #>    <int> <int> <int>         <dbl> #>  1  1969     2    11          7.56 #>  2  1969    10    31          7.25 #>  3  1969     3    26          8.88 #>  4  1969     1    11          7.75 #>  5  1969    10    10          8.62 #>  6  1969     9     4          6.25 #>  7  1969    11    16          7.19 #>  8  1969     3    10          8.50 #>  9  1969    10    29          6.44 #> 10  1969     8    26          8.56"},{"path":"https://bigrquery.r-dbi.org/dev/index.html","id":"dbi","dir":"","previous_headings":"Usage","what":"DBI","title":"An Interface to Googles BigQuery API'","text":"","code":"library(DBI)  con <- dbConnect(   bigrquery::bigquery(),   project = \"publicdata\",   dataset = \"samples\",   billing = billing ) con  #> <BigQueryConnection> #>   Dataset: publicdata.samples #>   Billing: gargle-169921  dbListTables(con) #> [1] \"github_nested\"   \"github_timeline\" \"gsod\"            \"natality\"        #> [5] \"shakespeare\"     \"trigrams\"        \"wikipedia\"  dbGetQuery(con, sql, n = 10) #> # A tibble: 10 × 4 #>     year month   day weight_pounds #>    <int> <int> <int>         <dbl> #>  1  1969     2    11          7.56 #>  2  1969    10    31          7.25 #>  3  1969     3    26          8.88 #>  4  1969     1    11          7.75 #>  5  1969    10    10          8.62 #>  6  1969     9     4          6.25 #>  7  1969    11    16          7.19 #>  8  1969     3    10          8.50 #>  9  1969    10    29          6.44 #> 10  1969     8    26          8.56"},{"path":"https://bigrquery.r-dbi.org/dev/index.html","id":"dplyr","dir":"","previous_headings":"Usage","what":"dplyr","title":"An Interface to Googles BigQuery API'","text":"","code":"library(dplyr)  natality <- tbl(con, \"natality\")  natality %>%   select(year, month, day, weight_pounds) %>%    head(10) %>%   collect() #> # A tibble: 10 × 4 #>     year month   day weight_pounds #>    <int> <int> <int>         <dbl> #>  1  2005    11    NA          8.88 #>  2  2005     1    NA          8.69 #>  3  2005     3    NA          7.08 #>  4  2005     7    NA          7.81 #>  5  2005     1    NA          8.56 #>  6  2005     1    NA          8.13 #>  7  2005     7    NA          8.50 #>  8  2005     9    NA          7.56 #>  9  2005     9    NA          8.14 #> 10  2005     4    NA          7.05"},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/index.html","id":"bigquery-account","dir":"","previous_headings":"Important details","what":"BigQuery account","title":"An Interface to Googles BigQuery API'","text":"use bigrquery, ’ll need BigQuery project. Fortunately, just want play around BigQuery API, ’s easy start Google’s free public data BigQuery sandbox. gives fun data play along enough free compute (1 TB queries & 10 GB storage per month) learn ropes. get started, open https://console.cloud.google.com/bigquery create project. Make note “Project ID” ’ll use billing project whenever work free sample data; project work data.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/index.html","id":"authentication-and-authorization","dir":"","previous_headings":"Important details","what":"Authentication and authorization","title":"An Interface to Googles BigQuery API'","text":"using bigrquery interactively, ’ll prompted authorize bigrquery browser. ’ll asked want cache tokens reuse future sessions. non-interactive usage, preferred use service account token, possible. places learn auth: Help bigrquery::bq_auth(). bigrquery obtains token gargle::token_fetch(), supports variety token flows. article provides full details, take advantage Application Default Credentials service accounts GCE VMs. Non-interactive auth. Explains set project code must run without user interaction. get API credentials. Instructions getting OAuth client service account token. Note bigrquery requests permission modify data; never unless explicitly request (e.g. calling bq_table_delete() bq_table_upload()). Privacy policy provides info.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/index.html","id":"useful-links","dir":"","previous_headings":"","what":"Useful links","title":"An Interface to Googles BigQuery API'","text":"SQL reference API reference Query/job console Billing console","code":""},{"path":"https://bigrquery.r-dbi.org/dev/index.html","id":"policies","dir":"","previous_headings":"","what":"Policies","title":"An Interface to Googles BigQuery API'","text":"Please note ‘bigrquery’ project released Contributor Code Conduct. contributing project, agree abide terms. Privacy policy","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/DBI.html","id":null,"dir":"Reference","previous_headings":"","what":"DBI methods — DBI","title":"DBI methods — DBI","text":"Implementations pure virtual functions defined DBI package.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/DBI.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBI methods — DBI","text":"","code":"# S4 method for class 'bq_dataset' dbConnect(drv, ...)  # S4 method for class 'BigQueryDriver' show(object)  # S4 method for class 'BigQueryDriver' dbGetInfo(dbObj, ...)  # S4 method for class 'BigQueryDriver' dbIsValid(dbObj, ...)  # S4 method for class 'BigQueryDriver' dbDataType(dbObj, obj, ...)  # S4 method for class 'BigQueryConnection' show(object)  # S4 method for class 'BigQueryConnection' dbIsValid(dbObj, ...)  # S4 method for class 'BigQueryConnection' dbDisconnect(conn, ...)  # S4 method for class 'BigQueryConnection,character' dbSendQuery(conn, statement, ..., params = NULL)  # S4 method for class 'BigQueryConnection,character' dbExecute(conn, statement, ...)  # S4 method for class 'BigQueryConnection,character' dbQuoteString(conn, x, ...)  # S4 method for class 'BigQueryConnection,SQL' dbQuoteString(conn, x, ...)  # S4 method for class 'BigQueryConnection,character' dbQuoteIdentifier(conn, x, ...)  # S4 method for class 'BigQueryConnection,SQL' dbQuoteIdentifier(conn, x, ...)  # S4 method for class 'BigQueryConnection,logical' dbQuoteLiteral(conn, x, ...)  # S4 method for class 'BigQueryConnection' dbDataType(dbObj, obj, ...)  # S4 method for class 'BigQueryConnection,character,data.frame' dbWriteTable(   conn,   name,   value,   ...,   overwrite = FALSE,   append = FALSE,   field.types = NULL,   temporary = FALSE,   row.names = NA )  # S4 method for class 'BigQueryConnection,Id,data.frame' dbWriteTable(   conn,   name,   value,   ...,   overwrite = FALSE,   append = FALSE,   field.types = NULL,   temporary = FALSE,   row.names = NA )  # S4 method for class 'BigQueryConnection,AsIs,data.frame' dbWriteTable(   conn,   name,   value,   ...,   overwrite = FALSE,   append = FALSE,   field.types = NULL,   temporary = FALSE,   row.names = NA )  # S4 method for class 'BigQueryConnection,character,data.frame' dbAppendTable(conn, name, value, ..., row.names = NULL)  # S4 method for class 'BigQueryConnection,Id,data.frame' dbAppendTable(conn, name, value, ..., row.names = NULL)  # S4 method for class 'BigQueryConnection,AsIs,data.frame' dbAppendTable(conn, name, value, ..., row.names = NULL)  # S4 method for class 'BigQueryConnection' dbCreateTable(conn, name, fields, ..., row.names = NULL, temporary = FALSE)  # S4 method for class 'BigQueryConnection' dbCreateTable(conn, name, fields, ..., row.names = NULL, temporary = FALSE)  # S4 method for class 'BigQueryConnection,character' dbReadTable(conn, name, ...)  # S4 method for class 'BigQueryConnection,Id' dbReadTable(conn, name, ...)  # S4 method for class 'BigQueryConnection,AsIs' dbReadTable(conn, name, ...)  # S4 method for class 'BigQueryConnection' dbListTables(conn, ...)  # S4 method for class 'BigQueryConnection,character' dbExistsTable(conn, name, ...)  # S4 method for class 'BigQueryConnection,Id' dbExistsTable(conn, name, ...)  # S4 method for class 'BigQueryConnection,AsIs' dbExistsTable(conn, name, ...)  # S4 method for class 'BigQueryConnection,character' dbListFields(conn, name, ...)  # S4 method for class 'BigQueryConnection,Id' dbListFields(conn, name, ...)  # S4 method for class 'BigQueryConnection,AsIs' dbListFields(conn, name, ...)  # S4 method for class 'BigQueryConnection,character' dbRemoveTable(conn, name, ...)  # S4 method for class 'BigQueryConnection,Id' dbRemoveTable(conn, name, ...)  # S4 method for class 'BigQueryConnection,AsIs' dbRemoveTable(conn, name, ...)  # S4 method for class 'BigQueryConnection' dbGetInfo(dbObj, ...)  # S4 method for class 'BigQueryConnection' dbBegin(conn, ...)  # S4 method for class 'BigQueryConnection' dbCommit(conn, ...)  # S4 method for class 'BigQueryConnection' dbRollback(conn, ...)  # S4 method for class 'BigQueryResult' show(object)  # S4 method for class 'BigQueryResult' dbIsValid(dbObj, ...)  # S4 method for class 'BigQueryResult' dbClearResult(res, ...)  # S4 method for class 'BigQueryResult' dbFetch(res, n = -1, ...)  # S4 method for class 'BigQueryResult' dbHasCompleted(res, ...)  # S4 method for class 'BigQueryResult' dbGetStatement(res, ...)  # S4 method for class 'BigQueryResult' dbColumnInfo(res, ...)  # S4 method for class 'BigQueryResult' dbGetRowCount(res, ...)  # S4 method for class 'BigQueryResult' dbGetRowsAffected(res, ...)  # S4 method for class 'BigQueryResult' dbBind(res, params, ...)"},{"path":"https://bigrquery.r-dbi.org/dev/reference/DBI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBI methods — DBI","text":"... arguments methods. object R object dbObj object inheriting DBI::DBIObject, .e. DBI::DBIDriver, DBI::DBIConnection, DBI::DBIResult. obj R object whose SQL type want determine. conn DBI::DBIConnection object, returned DBI::dbConnect(). statement character string containing SQL. params dbBind(), list values, named unnamed, data frame, one element/column per query parameter. dbBindArrow(), values nanoarrow stream, one column per query parameter. x character vector quote string. name table name, passed dbQuoteIdentifier(). Options : character string unquoted DBMS table name, e.g. \"table_name\", call Id() components fully qualified table name, e.g. Id(schema = \"my_schema\", table = \"table_name\") call SQL() quoted fully qualified table name given verbatim, e.g. SQL('\"my_schema\".\"table_name\"') value data.frame (coercible data.frame). overwrite logical specifying whether overwrite existing table . default FALSE. append logical specifying whether append existing table DBMS.  default FALSE. field.types, temporary Ignored. Included compatibility generic. row.names logical specifying whether row.names output output DBMS table; TRUE, extra field whose name whatever R identifier \"row.names\" maps DBMS (see DBI::make.db.names()). NA add rows names characters, otherwise ignore. fields Either character vector data frame. named character vector: Names column names, values types. Names escaped dbQuoteIdentifier(). Field types unescaped. data frame: field types generated using dbDataType(). res object inheriting DBI::DBIResult. n maximum number records retrieve per fetch. Use n = -1 n = Inf retrieve pending records.  implementations may recognize special values.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"BigQuery datasets — api-dataset","title":"BigQuery datasets — api-dataset","text":"Basic create-read-update-delete verbs datasets.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BigQuery datasets — api-dataset","text":"","code":"bq_dataset_create(x, location = \"US\", ...)  bq_dataset_meta(x, fields = NULL)  bq_dataset_exists(x)  bq_dataset_update(x, ...)  bq_dataset_delete(x, delete_contents = FALSE)  bq_dataset_tables(x, page_size = 50, max_pages = Inf, warn = TRUE, ...)"},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BigQuery datasets — api-dataset","text":"x bq_dataset location Dataset location ... Additional arguments passed underlying API call. snake_case names automatically converted camelCase. fields optional field specification partial response delete_contents TRUE, recursively delete tables dataset. Set FALSE default safety. page_size Number items per page. max_pages Maximum number pages retrieve. Use Inf retrieve pages (may take long time!) warn TRUE, warn unretrieved pages.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-dataset.html","id":"google-bigquery-api-documentation","dir":"Reference","previous_headings":"","what":"Google BigQuery API documentation","title":"BigQuery datasets — api-dataset","text":"get insert delete list","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"BigQuery datasets — api-dataset","text":"","code":"if (FALSE) { # bq_testable() ds <- bq_dataset(bq_test_project(), \"dataset_api\") bq_dataset_exists(ds)  bq_dataset_create(ds) bq_dataset_exists(ds) str(bq_dataset_meta(ds))  bq_dataset_delete(ds) bq_dataset_exists(ds)  # Use bq_test_dataset() to create a temporary dataset that will # be automatically deleted ds <- bq_test_dataset() bq_table_create(bq_table(ds, \"x1\")) bq_table_create(bq_table(ds, \"x2\")) bq_table_create(bq_table(ds, \"x3\")) bq_dataset_tables(ds) }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-job.html","id":null,"dir":"Reference","previous_headings":"","what":"BigQuery job: retrieve metadata — api-job","title":"BigQuery job: retrieve metadata — api-job","text":"perform job, see api-perform. functions retrieve metadata (various forms) existing job.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-job.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BigQuery job: retrieve metadata — api-job","text":"","code":"bq_job_meta(x, fields = NULL)  bq_job_status(x)  bq_job_show_statistics(x)  bq_job_wait(   x,   quiet = getOption(\"bigrquery.quiet\"),   pause = 0.5,   call = caller_env() )"},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-job.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BigQuery job: retrieve metadata — api-job","text":"x bq_job fields optional field specification partial response quiet FALSE, displays progress bar; TRUE silent; NA picks based whether interactive context. pause amount time wait status requests call execution environment currently running function, e.g. caller_env(). function mentioned error messages source error. See call argument abort() information.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-job.html","id":"google-bigquery-api-documentation","dir":"Reference","previous_headings":"","what":"Google BigQuery API documentation","title":"BigQuery job: retrieve metadata — api-job","text":"get","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-job.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"BigQuery job: retrieve metadata — api-job","text":"","code":"if (FALSE) { # bq_testable() jobs <- bq_project_jobs(bq_test_project()) jobs[[1]]  # Show statistics about job bq_job_show_statistics(jobs[[1]])  # Wait for job to complete bq_job_wait(jobs[[1]]) }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-perform.html","id":null,"dir":"Reference","previous_headings":"","what":"BigQuery jobs: perform a job — api-perform","title":"BigQuery jobs: perform a job — api-perform","text":"functions low-level functions designed used experts. low-level functions paired high-level function use instead: bq_perform_copy():    bq_table_copy(). bq_perform_query():   bq_dataset_query(), bq_project_query(). bq_perform_upload():  bq_table_upload(). bq_perform_load():    bq_table_load(). bq_perform_extract(): bq_table_save().","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-perform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BigQuery jobs: perform a job — api-perform","text":"","code":"bq_perform_extract(   x,   destination_uris,   destination_format = \"NEWLINE_DELIMITED_JSON\",   compression = \"NONE\",   ...,   print_header = TRUE,   billing = x$project )  bq_perform_upload(   x,   values,   fields = NULL,   source_format = c(\"NEWLINE_DELIMITED_JSON\", \"PARQUET\"),   create_disposition = \"CREATE_IF_NEEDED\",   write_disposition = \"WRITE_EMPTY\",   ...,   billing = x$project )  bq_perform_load(   x,   source_uris,   billing = x$project,   source_format = \"NEWLINE_DELIMITED_JSON\",   fields = NULL,   nskip = 0,   create_disposition = \"CREATE_IF_NEEDED\",   write_disposition = \"WRITE_EMPTY\",   ... )  bq_perform_query(   query,   billing,   ...,   parameters = NULL,   destination_table = NULL,   default_dataset = NULL,   create_disposition = \"CREATE_IF_NEEDED\",   write_disposition = \"WRITE_EMPTY\",   use_legacy_sql = FALSE,   priority = \"INTERACTIVE\" )  bq_perform_query_dry_run(   query,   billing,   ...,   default_dataset = NULL,   parameters = NULL,   use_legacy_sql = FALSE )  bq_perform_query_schema(   query,   billing,   ...,   default_dataset = NULL,   parameters = NULL )  bq_perform_copy(   src,   dest,   create_disposition = \"CREATE_IF_NEEDED\",   write_disposition = \"WRITE_EMPTY\",   ...,   billing = NULL )"},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-perform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BigQuery jobs: perform a job — api-perform","text":"x bq_table destination_uris character vector fully-qualified Google Cloud Storage URIs extracted table written. Can export 1 Gb data per file. Use wild card URI (e.g. gs://[YOUR_BUCKET]/file-name-*.json) automatically create number files. destination_format exported file format: CSV files, specify \"CSV\" (Nested repeated data supported). newline-delimited JSON, specify \"NEWLINE_DELIMITED_JSON\". Avro, specify \"AVRO\". parquet, specify \"PARQUET\". compression compression type use exported files: CSV files: \"GZIP\" \"NONE\". newline-delimited JSON: \"GZIP\" \"NONE\". Avro: \"DEFLATE\", \"SNAPPY\" \"NONE\". parquet: \"SNAPPY\", \"GZIP\", \"ZSTD\" \"NONE\". ... Additional arguments passed underlying API call. snake_case names automatically converted camelCase. print_header Whether print header row results. billing Identifier project bill. values Data frame values insert. fields bq_fields specification, something coercible (like data frame). Leave NULL allow BigQuery auto-detect fields. source_format format data files: CSV files, specify \"CSV\". datastore backups, specify \"DATASTORE_BACKUP\". newline-delimited JSON, specify \"NEWLINE_DELIMITED_JSON\". Avro, specify \"AVRO\". parquet, specify \"PARQUET\". orc, specify \"ORC\". create_disposition Specifies whether job allowed create new tables. following values supported: \"CREATE_IF_NEEDED\": table exist, BigQuery creates table. \"CREATE_NEVER\": table must already exist. , 'notFound' error returned job result. write_disposition Specifies action occurs destination table already exists. following values supported: \"WRITE_TRUNCATE\": table already exists, BigQuery overwrites table data. \"WRITE_APPEND\": table already exists, BigQuery appends data table. \"WRITE_EMPTY\": table already exists contains data, 'duplicate' error returned job result. source_uris fully-qualified URIs point data Google Cloud. Google Cloud Storage URIs: URI can contain one '*' wildcard character must come 'bucket' name. Size limits related load jobs apply external data sources. Google Cloud Bigtable URIs: Exactly one URI can specified fully specified valid HTTPS URL Google Cloud Bigtable table. Google Cloud Datastore backups: Exactly one URI can specified. Also, '*' wildcard character allowed. nskip source_format = \"CSV\", number header rows skip. query SQL query string. parameters Named list parameters match query parameters. Parameter x matched placeholder @x. Generally, can supply R vectors automatically converted correct type. need greater control, can call bq_param_scalar() bq_param_array() explicitly. See https://cloud.google.com/bigquery/docs/parameterized-queries details. destination_table bq_table results stored. supplied, results saved temporary table lives special dataset. must supply parameter large queries (> 128 MB compressed). default_dataset bq_dataset used automatically qualify table names. use_legacy_sql TRUE use BigQuery's legacy SQL format. priority Specifies priority query. Possible values include \"INTERACTIVE\" \"BATCH\". Batch queries start immediately, rate-limited way interactive queries.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-perform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BigQuery jobs: perform a job — api-perform","text":"bq_job.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-perform.html","id":"google-bigquery-api-documentation","dir":"Reference","previous_headings":"","what":"Google BigQuery API documentation","title":"BigQuery jobs: perform a job — api-perform","text":"jobs Additional information : exporting data loading data writing queries copying table","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-perform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"BigQuery jobs: perform a job — api-perform","text":"","code":"if (FALSE) { # bq_testable() ds <- bq_test_dataset() bq_mtcars <- bq_table(ds, \"mtcars\") job <- bq_perform_upload(bq_mtcars, mtcars) bq_table_exists(bq_mtcars)  bq_job_wait(job) bq_table_exists(bq_mtcars) head(bq_table_download(bq_mtcars)) }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-project.html","id":null,"dir":"Reference","previous_headings":"","what":"BigQuery project methods — api-project","title":"BigQuery project methods — api-project","text":"Projects two primary components: datasets jobs. Unlike BigQuery objects, accompanying bq_project S3 class project simple string.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-project.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BigQuery project methods — api-project","text":"","code":"bq_project_datasets(x, page_size = 100, max_pages = 1, warn = TRUE)  bq_project_jobs(x, page_size = 100, max_pages = 1, warn = TRUE)"},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-project.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BigQuery project methods — api-project","text":"x string giving project name. page_size Number items per page. max_pages Maximum number pages retrieve. Use Inf retrieve pages (may take long time!) warn TRUE, warn unretrieved pages.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-project.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BigQuery project methods — api-project","text":"bq_project_datasets(): list bq_datasets bq_project_jobs(): list bq_jobs.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-project.html","id":"google-bigquery-api-documentation","dir":"Reference","previous_headings":"","what":"Google BigQuery API documentation","title":"BigQuery project methods — api-project","text":"datasets jobs One day might also expose general project metadata.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-project.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"BigQuery project methods — api-project","text":"","code":"if (FALSE) { # bq_testable() bq_project_datasets(\"bigquery-public-data\") bq_project_datasets(\"githubarchive\")  bq_project_jobs(bq_test_project(), page_size = 10) }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-table.html","id":null,"dir":"Reference","previous_headings":"","what":"BigQuery tables — api-table","title":"BigQuery tables — api-table","text":"Basic create-read-update-delete verbs tables, well functions uploading data (bq_table_upload()), saving /loading Google Cloud Storage (bq_table_load(), bq_table_save()), getting various values metadata.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BigQuery tables — api-table","text":"","code":"bq_table_create(x, fields = NULL, ...)  bq_table_meta(x, fields = NULL)  bq_table_fields(x)  bq_table_size(x)  bq_table_nrow(x)  bq_table_exists(x)  bq_table_delete(x)  bq_table_copy(x, dest, ..., quiet = NA)  bq_table_upload(x, values, ..., quiet = NA)  bq_table_save(x, destination_uris, ..., quiet = NA)  bq_table_load(x, source_uris, ..., quiet = NA)  bq_table_patch(x, fields)"},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BigQuery tables — api-table","text":"x bq_table, object coercible bq_table. fields bq_fields specification, something coercible (like data frame). ... Additional arguments passed underlying API call. snake_case names automatically converted camelCase. dest Source destination bq_tables. quiet FALSE, displays progress bar; TRUE silent; NA picks based whether interactive context. values Data frame values insert. destination_uris character vector fully-qualified Google Cloud Storage URIs extracted table written. Can export 1 Gb data per file. Use wild card URI (e.g. gs://[YOUR_BUCKET]/file-name-*.json) automatically create number files. source_uris fully-qualified URIs point data Google Cloud. Google Cloud Storage URIs: URI can contain one '*' wildcard character must come 'bucket' name. Size limits related load jobs apply external data sources. Google Cloud Bigtable URIs: Exactly one URI can specified fully specified valid HTTPS URL Google Cloud Bigtable table. Google Cloud Datastore backups: Exactly one URI can specified. Also, '*' wildcard character allowed.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BigQuery tables — api-table","text":"bq_table_copy(), bq_table_create(), bq_table_delete(), bq_table_upload(): invisible bq_table bq_table_exists(): either TRUE FALSE. bq_table_size(): size table bytes bq_table_fields(): bq_fields.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-table.html","id":"google-bigquery-api-documentation","dir":"Reference","previous_headings":"","what":"Google BigQuery API documentation","title":"BigQuery tables — api-table","text":"insert get delete","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/api-table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"BigQuery tables — api-table","text":"","code":"if (FALSE) { # bq_testable() ds <- bq_test_dataset()  bq_mtcars <- bq_table(ds, \"mtcars\") bq_table_exists(bq_mtcars)  bq_table_create(   bq_mtcars,   fields = mtcars,   friendly_name = \"Motor Trend Car Road Tests\",   description = \"The data was extracted from the 1974 Motor Trend US magazine\",   labels = list(category = \"example\") ) bq_table_exists(bq_mtcars)  bq_table_upload(bq_mtcars, mtcars)  bq_table_fields(bq_mtcars) bq_table_size(bq_mtcars) str(bq_table_meta(bq_mtcars))  bq_table_delete(bq_mtcars) bq_table_exists(bq_mtcars)  my_natality <- bq_table(ds, \"mynatality\") bq_table_copy(\"publicdata.samples.natality\", my_natality) }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bigquery.html","id":null,"dir":"Reference","previous_headings":"","what":"BigQuery DBI driver — bigquery","title":"BigQuery DBI driver — bigquery","text":"Creates BigQuery DBI driver use DBI::dbConnect().","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bigquery.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BigQuery DBI driver — bigquery","text":"","code":"# S4 method for class 'BigQueryDriver' dbConnect(   drv,   project,   dataset = NULL,   billing = project,   page_size = 10000,   quiet = NA,   use_legacy_sql = FALSE,   bigint = c(\"integer\", \"integer64\", \"numeric\", \"character\"),   ... )"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bigquery.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BigQuery DBI driver — bigquery","text":"drv result bigquery(). project, dataset Project dataset identifiers billing Identifier project bill. page_size Number items per page. quiet FALSE, displays progress bar; TRUE silent; NA picks based whether interactive context. use_legacy_sql TRUE use BigQuery's legacy SQL format. bigint R type BigQuery's 64-bit integer types mapped . default \"integer\" returns R's integer type results NA values /+/- 2147483647. \"integer64\" returns bit64::integer64, allows full range 64 bit integers. ... arguments compatibility generic; currently ignored.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bigquery.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"BigQuery DBI driver — bigquery","text":"","code":"if (FALSE) { # bq_testable() con <- DBI::dbConnect(   bigquery(),   project = \"publicdata\",   dataset = \"samples\",   billing = bq_test_project() ) con DBI::dbListTables(con) DBI::dbReadTable(con, \"natality\", n_max = 10)  # Create a temporary dataset to explore ds <- bq_test_dataset() con <- DBI::dbConnect(   bigquery(),   project = ds$project,   dataset = ds$dataset ) DBI::dbWriteTable(con, \"mtcars\", mtcars) DBI::dbReadTable(con, \"mtcars\")[1:6, ]  DBI::dbGetQuery(con, \"SELECT count(*) FROM mtcars\")  res <- DBI::dbSendQuery(con, \"SELECT cyl, mpg FROM mtcars\") dbColumnInfo(res) dbFetch(res, 10) dbFetch(res, -1) DBI::dbHasCompleted(res) }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bigrquery-package.html","id":null,"dir":"Reference","previous_headings":"","what":"bigrquery: An Interface to Google's 'BigQuery' 'API' — bigrquery-package","title":"bigrquery: An Interface to Google's 'BigQuery' 'API' — bigrquery-package","text":"Easily talk Google's 'BigQuery' database R.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bigrquery-package.html","id":"package-options","dir":"Reference","previous_headings":"","what":"Package options","title":"bigrquery: An Interface to Google's 'BigQuery' 'API' — bigrquery-package","text":"bigrquery.quiet Verbose output processing? default value, NA, turns verbose output queries run longer two seconds.  Use FALSE immediate verbose output, TRUE quiet operation. bigrquery.page.size Default page size fetching data, defaults 1e4.","code":""},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/reference/bigrquery-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"bigrquery: An Interface to Google's 'BigQuery' 'API' — bigrquery-package","text":"Maintainer: Hadley Wickham hadley@posit.co (ORCID) Authors: Jennifer Bryan jenny@posit.co (ORCID) contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth.html","id":null,"dir":"Reference","previous_headings":"","what":"Authorize bigrquery — bq_auth","title":"Authorize bigrquery — bq_auth","text":"Authorize bigrquery view manage BigQuery projects. function wrapper around gargle::token_fetch(). default, directed web browser, asked sign Google account, grant bigrquery permission operate behalf Google BigQuery. default, permission, user credentials cached folder home directory, can automatically refreshed, necessary. Storage user level means token can used across multiple projects tokens less likely synced cloud accident.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Authorize bigrquery — bq_auth","text":"","code":"bq_auth(   email = gargle::gargle_oauth_email(),   path = NULL,   scopes = c(\"https://www.googleapis.com/auth/bigquery\",     \"https://www.googleapis.com/auth/cloud-platform\"),   cache = gargle::gargle_oauth_cache(),   use_oob = gargle::gargle_oob_default(),   token = NULL )"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Authorize bigrquery — bq_auth","text":"email Optional. specified, email can take several different forms: \"jane@gmail.com\", .e. actual email address. allows user target specific Google identity. specified, used token lookup, .e. determine suitable token already available cache. token found, email used pre-select targeted Google identity OAuth chooser. (Note, however, email associated token cached always determined token , never argument). \"*@example.com\", .e. domain-glob pattern. can helpful need code \"just works\" alice@example.com bob@example.com. TRUE means approving email auto-discovery. exactly one matching token found cache, used. FALSE NA mean want ignore token cache force new OAuth dance browser. Defaults option named \"gargle_oauth_email\", retrieved gargle_oauth_email() (unless wrapper package implements different default behavior). path JSON identifying service account, one forms supported txt argument jsonlite::fromJSON() (typically, file path JSON string). scopes character vector scopes request. Pick listed https://developers.google.com/identity/protocols/oauth2/scopes. cache Specifies OAuth token cache. Defaults option named \"gargle_oauth_cache\", retrieved via gargle_oauth_cache(). use_oob Whether use --band authentication (, perhaps, variant implemented gargle known \"pseudo-OOB\") first acquiring token. Defaults value returned gargle_oob_default(). Note (pseudo-)OOB auth affects initial OAuth dance. retrieve (possibly refresh) cached token, use_oob effect. OAuth client provided implicitly wrapper package, type probably defaults value returned gargle_oauth_client_type(). can take control client type setting options(gargle_oauth_client_type = \"web\") options(gargle_oauth_client_type = \"installed\"). token token class Token2.0 object httr's class request, .e. token prepared httr::config() Token2.0 auth_token component.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Authorize bigrquery — bq_auth","text":"users, time, need call bq_auth() explicitly – triggered first action requires authorization. Even called, default arguments often suffice. However, necessary, bq_auth() allows user explicitly: Declare Google identity use, via email specification. Use service account token workload identity federation via path. Bring token. Customize scopes. Use non-default cache folder turn caching . Explicitly request --band (OOB) auth via use_oob. interacting R within browser (applies RStudio Server, Posit Workbench, Posit Cloud, Google Colaboratory), need OOB auth pseudo-OOB variant. happen automatically, can request explicitly use_oob = TRUE , persistently, setting option via options(gargle_oob_default = TRUE). choice conventional OOB pseudo-OOB auth determined type OAuth client. client \"installed\" type, use_oob = TRUE results conventional OOB auth. client \"web\" type, use_oob = TRUE results pseudo-OOB auth. Packages provide built-OAuth client can usually detect type client use. need set explicitly, use \"gargle_oauth_client_type\" option:   details many ways find token, see gargle::token_fetch(). deeper control auth, use bq_auth_configure() bring OAuth client API key. learn gargle options, see gargle::gargle_options.","code":"options(gargle_oauth_client_type = \"web\")       # pseudo-OOB # or, alternatively options(gargle_oauth_client_type = \"installed\") # conventional OOB"},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Authorize bigrquery — bq_auth","text":"","code":"if (FALSE) { # \\dontrun{ ## load/refresh existing credentials, if available ## otherwise, go to browser for authentication and authorization bq_auth()  ## force use of a token associated with a specific email bq_auth(email = \"jenny@example.com\")  ## force a menu where you can choose from existing tokens or ## choose to get a new one bq_auth(email = NA)  ## use a 'read only' scope, so it's impossible to change data bq_auth(   scopes = \"https://www.googleapis.com/auth/devstorage.read_only\" )  ## use a service account token bq_auth(path = \"foofy-83ee9e7c9c48.json\") } # }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth_configure.html","id":null,"dir":"Reference","previous_headings":"","what":"Edit and view auth configuration — bq_auth_configure","title":"Edit and view auth configuration — bq_auth_configure","text":"functions give control visibility auth configuration bq_auth() . bq_auth_configure() lets user specify : OAuth client, used obtaining user token. See vignette(\"get-api-credentials\", package = \"gargle\") . user configure settings, internal defaults used. bq_oauth_client() retrieves currently configured OAuth client.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth_configure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Edit and view auth configuration — bq_auth_configure","text":"","code":"bq_auth_configure(client, path, app = deprecated())  bq_oauth_client()"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth_configure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Edit and view auth configuration — bq_auth_configure","text":"client Google OAuth client, presumably constructed via gargle::gargle_oauth_client_from_json(). Note, however, preferred specify client JSON, using path argument. path JSON downloaded Google Cloud Console, containing client id secret, one forms supported txt argument jsonlite::fromJSON() (typically, file path JSON string). app Replaced client argument.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth_configure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Edit and view auth configuration — bq_auth_configure","text":"bq_auth_configure(): object R6 class gargle::AuthState, invisibly. bq_oauth_client(): current user-configured OAuth client.","code":""},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_auth_configure.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Edit and view auth configuration — bq_auth_configure","text":"","code":"# see and store the current user-configured OAuth client (probably `NULL`) (original_client <- bq_oauth_client()) #> NULL  # the preferred way to configure your own client is via a JSON file # downloaded from Google Developers Console # this example JSON is indicative, but fake path_to_json <- system.file(   \"extdata\", \"data\", \"client_secret_123.googleusercontent.com.json\",   package = \"bigrquery\" ) bq_auth_configure(path = path_to_json)  # confirm the changes bq_oauth_client() #> <gargle_oauth_client> #> name: a_project_d1c5a8066d2cbe48e8d94514dd286163 #> id: abc.apps.googleusercontent.com #> secret: <REDACTED> #> type: installed #> redirect_uris: http://localhost  # restore original auth config bq_auth_configure(client = original_client)"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_deauth.html","id":null,"dir":"Reference","previous_headings":"","what":"Clear current token — bq_deauth","title":"Clear current token — bq_deauth","text":"Clears currently stored token. next time bigrquery needs token, token acquisition process starts , fresh call bq_auth() , therefore, internally, call gargle::token_fetch(). Unlike packages use gargle, bigrquery usable de-authorized state. Therefore, calling bq_deauth() clears token, .e. imply subsequent requests made API key lieu token.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_deauth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clear current token — bq_deauth","text":"","code":"bq_deauth()"},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_deauth.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clear current token — bq_deauth","text":"","code":"if (FALSE) { # \\dontrun{ bq_deauth() } # }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_field.html","id":null,"dir":"Reference","previous_headings":"","what":"BigQuery field (and fields) class — bq_field","title":"BigQuery field (and fields) class — bq_field","text":"bq_field() bq_fields() create; as_bq_field() as_bq_fields() coerce lists.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_field.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BigQuery field (and fields) class — bq_field","text":"","code":"bq_field(name, type, mode = \"NULLABLE\", fields = list(), description = NULL)  bq_fields(x)  as_bq_field(x)  as_bq_fields(x)"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_field.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BigQuery field (and fields) class — bq_field","text":"name field name. name must contain letters (-z, -Z), numbers (0-9), underscores (_), must start letter underscore. maximum length 300 characters. type field data type. Possible values include: \"STRING\", \"BYTES\", \"INTEGER\", \"FLOAT\", \"BOOLEAN\", \"TIMESTAMP\", \"DATE\", \"TIME\", \"DATETIME\", \"GEOGRAPHY\", \"NUMERIC\", \"BIGNUMERIC\", \"JSON\", \"RECORD\". mode field mode. Possible values include: \"NULLABLE\", \"REQUIRED\", \"REPEATED\". fields field type \"record\", list sub-fields. description field description. maximum length 1,024 characters. x list bg_fields","code":""},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_field.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"BigQuery field (and fields) class — bq_field","text":"","code":"bq_field(\"name\", \"string\") #> <bq_field> name <STRING> #>   as_bq_fields(list(   list(name = \"name\", type = \"string\"),   bq_field(\"age\", \"integer\") )) #> <bq_fields> #>   name <STRING> #>   age <INTEGER> #>   # as_bq_fields() can also take a data frame as_bq_fields(mtcars) #> <bq_fields> #>   mpg <FLOAT> #>   cyl <FLOAT> #>   disp <FLOAT> #>   hp <FLOAT> #>   drat <FLOAT> #>   wt <FLOAT> #>   qsec <FLOAT> #>   vs <FLOAT> #>   am <FLOAT> #>   gear <FLOAT> #>   carb <FLOAT> #>"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_has_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Is there a token on hand? — bq_has_token","title":"Is there a token on hand? — bq_has_token","text":"Reports whether bigrquery stored token, ready use downstream requests.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_has_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Is there a token on hand? — bq_has_token","text":"","code":"bq_has_token()"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_has_token.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Is there a token on hand? — bq_has_token","text":"Logical.","code":""},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_has_token.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Is there a token on hand? — bq_has_token","text":"","code":"bq_has_token() #> [1] FALSE"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_oauth_app.html","id":null,"dir":"Reference","previous_headings":"","what":"Get currently configured OAuth app (deprecated) — bq_oauth_app","title":"Get currently configured OAuth app (deprecated) — bq_oauth_app","text":"light new gargle::gargle_oauth_client() constructor class name, bq_oauth_app() replaced bq_oauth_client().","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_oauth_app.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get currently configured OAuth app (deprecated) — bq_oauth_app","text":"","code":"bq_oauth_app()"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_param.html","id":null,"dir":"Reference","previous_headings":"","what":"Explicitly define query parameters — bq_param","title":"Explicitly define query parameters — bq_param","text":"default, bigrquery assume vectors length 1 scalars, longer vectors arrays. need pass length-1 array, need explicitly use bq_param_array().","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_param.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explicitly define query parameters — bq_param","text":"","code":"bq_param(value, type = NULL, name = NULL)  bq_param_scalar(value, type = NULL, name = NULL)  bq_param_array(value, type = NULL, name = NULL)"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_param.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explicitly define query parameters — bq_param","text":"value vector parameter values type BigQuery type parameter name name parameter query, omitting @","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_param.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explicitly define query parameters — bq_param","text":"","code":"# bq_param() automatically picks scalar vs array based on length bq_param(\"a\") #> { #>   \"name\": {}, #>   \"parameterType\": { #>     \"type\": \"STRING\" #>   }, #>   \"parameterValue\": { #>     \"value\": \"a\" #>   } #> } bq_param(c(\"a\", \"b\", \"c\")) #> { #>   \"name\": {}, #>   \"parameterType\": { #>     \"type\": \"ARRAY\", #>     \"arrayType\": { #>       \"type\": \"STRING\" #>     } #>   }, #>   \"parameterValue\": { #>     \"arrayValues\": [ #>       { #>         \"value\": \"a\" #>       }, #>       { #>         \"value\": \"b\" #>       }, #>       { #>         \"value\": \"c\" #>       } #>     ] #>   } #> }  # use bq_param_array() to create a length-1 array bq_param_array(\"a\") #> { #>   \"name\": {}, #>   \"parameterType\": { #>     \"type\": \"ARRAY\", #>     \"arrayType\": { #>       \"type\": \"STRING\" #>     } #>   }, #>   \"parameterValue\": { #>     \"arrayValues\": [ #>       { #>         \"value\": \"a\" #>       } #>     ] #>   } #> }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_projects.html","id":null,"dir":"Reference","previous_headings":"","what":"List available projects — bq_projects","title":"List available projects — bq_projects","text":"List projects access . can also work public datasets, need provide billing project whenever perform non-free operation.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_projects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List available projects — bq_projects","text":"","code":"bq_projects(page_size = 100, max_pages = 1, warn = TRUE)"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_projects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List available projects — bq_projects","text":"page_size Number items per page. max_pages Maximum number pages retrieve. Use Inf retrieve pages (may take long time!) warn TRUE, warn unretrieved pages.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_projects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List available projects — bq_projects","text":"character vector.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_projects.html","id":"google-bigquery-api-documentation","dir":"Reference","previous_headings":"","what":"Google BigQuery API documentation","title":"List available projects — bq_projects","text":"list","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_projects.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List available projects — bq_projects","text":"","code":"if (FALSE) { # bq_testable() bq_projects() }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit query to BigQuery — bq_query","title":"Submit query to BigQuery — bq_query","text":"submit query (using bq_perform_query()) wait complete (bq_job_wait()). BigQuery queries save results table (temporary otherwise), functions return bq_table can query information.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit query to BigQuery — bq_query","text":"","code":"bq_project_query(x, query, destination_table = NULL, ..., quiet = NA)  bq_dataset_query(   x,   query,   destination_table = NULL,   ...,   billing = NULL,   quiet = NA )"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit query to BigQuery — bq_query","text":"x Either project (string) bq_dataset. query SQL query string. destination_table bq_table results stored. supplied, results saved temporary table lives special dataset. must supply parameter large queries (> 128 MB compressed). ... Passed bq_perform_query() quiet FALSE, displays progress bar; TRUE silent; NA picks based whether interactive context. billing query dataset read access , public dataset, must also submit billing project.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit query to BigQuery — bq_query","text":"bq_table","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit query to BigQuery — bq_query","text":"","code":"if (FALSE) { # bq_testable() # Querying a project requires full name in query tb <- bq_project_query(   bq_test_project(),   \"SELECT count(*) FROM publicdata.samples.natality\" ) bq_table_fields(tb) bq_table_download(tb)  # Querying a dataset sets default dataset so you can use bare table name, # but for public data, you'll need to set a project to bill. ds <- bq_dataset(\"publicdata\", \"samples\") tb <- bq_dataset_query(ds,   query = \"SELECT count(*) FROM natality\",   billing = bq_test_project() ) bq_table_download(tb)  tb <- bq_dataset_query(ds,   query = \"SELECT count(*) FROM natality WHERE state = @state\",   parameters = list(state = \"KS\"),   billing = bq_test_project() ) bq_table_download(tb) }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_refs.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 classes for BigQuery datasets, tables and jobs — bq_refs","title":"S3 classes for BigQuery datasets, tables and jobs — bq_refs","text":"Create references BigQuery datasets, jobs, tables. class constructor function (bq_dataset(), bq_table(), bq_job()) coercion function (as_bq_dataset(), as_bq_table(), as_bq_job()). coercions functions come methods strings (find components splitting .), lists (look named components like projectId project_id). bq_table_, bq_dataset_ bq_job_ functions call appropriate coercion functions first argument, allowing flexible specify inputs.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_refs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 classes for BigQuery datasets, tables and jobs — bq_refs","text":"","code":"bq_dataset(project, dataset)  as_bq_dataset(x, ..., error_arg = caller_arg(x), error_call = caller_env())  bq_table(project, dataset, table = NULL, type = \"TABLE\")  as_bq_table(x, ..., error_arg = caller_arg(x), error_call = caller_env())  bq_job(project, job, location = \"US\")  as_bq_job(x, ..., error_arg = caller_arg(x), error_call = caller_env())"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_refs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 classes for BigQuery datasets, tables and jobs — bq_refs","text":"project, dataset, table, job, type Individual project, dataset, table, job identifiers table type (strings). bq_table(), supply bq_dataset first argument, 2nd argument interpreted table x object coerce bq_job, bq_dataset, bq_table. Built-methods handle strings lists. ... arguments passed methods. error_arg argument name string. argument mentioned error messages input origin problem. error_call execution environment currently running function, e.g. caller_env(). function mentioned error messages source error. See call argument abort() information. location Job location","code":""},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_refs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"S3 classes for BigQuery datasets, tables and jobs — bq_refs","text":"","code":"# Creation ------------------------------------------------ samples <- bq_dataset(\"publicdata\", \"samples\") natality <- bq_table(\"publicdata\", \"samples\", \"natality\") natality #> <bq_table> publicdata.samples.natality  # Or bq_table(samples, \"natality\") #> <bq_table> publicdata.samples.natality  bq_job(\"bigrquery-examples\", \"m0SgFu2ycbbge6jgcvzvflBJ_Wft\") #> <bq_job> bigrquery-examples.m0SgFu2ycbbge6jgcvzvflBJ_Wft.US  # Coercion ------------------------------------------------ as_bq_dataset(\"publicdata.shakespeare\") #> <bq_dataset> publicdata.shakespeare as_bq_table(\"publicdata.samples.natality\") #> <bq_table> publicdata.samples.natality  as_bq_table(list(   project_id = \"publicdata\",   dataset_id = \"samples\",   table_id = \"natality\" )) #> <bq_table> publicdata.samples.natality  as_bq_job(list(   projectId = \"bigrquery-examples\",   jobId = \"job_m0SgFu2ycbbge6jgcvzvflBJ_Wft\",   location = \"US\" )) #> <bq_job> bigrquery-examples.job_m0SgFu2ycbbge6jgcvzvflBJ_Wft.US"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_table_download.html","id":null,"dir":"Reference","previous_headings":"","what":"Download table data — bq_table_download","title":"Download table data — bq_table_download","text":"function provides two ways download data BigQuery, transfering data using either JSON arrow, depending api argument. bigrquerystorage installed, api = \"arrow\" used (much faster, see limitions ), otherwise can select deliberately using api = \"json\" api = \"arrow\".","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_table_download.html","id":"arrow-api","dir":"Reference","previous_headings":"","what":"Arrow API","title":"Download table data — bq_table_download","text":"arrow API much faster, heavier dependencies: bigrquerystorage requires arrow package, can tricky compile Linux (usually able get binary Posit Public Package Manager. one known limitation api = \"arrow\": querying public data, now need provide billing project.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_table_download.html","id":"json-api","dir":"Reference","previous_headings":"","what":"JSON API","title":"Download table data — bq_table_download","text":"JSON API retrieves rows chunks page_size. suitable results smaller queries (<100 MB, say). Unfortunately due limitations BigQuery API, may need vary parameter depending complexity underlying data. JSON API convert nested repeated columns list-columns follows: Repeated values (arrays) become list-column vectors. Records become list-columns named lists. Repeated records become list-columns data frames.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_table_download.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download table data — bq_table_download","text":"","code":"bq_table_download(   x,   n_max = Inf,   page_size = NULL,   start_index = 0L,   max_connections = 6L,   quiet = NA,   bigint = c(\"integer\", \"integer64\", \"numeric\", \"character\"),   api = c(\"json\", \"arrow\"),   billing = x$project,   max_results = deprecated() )"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_table_download.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download table data — bq_table_download","text":"x bq_table n_max Maximum number results retrieve. Use Inf retrieve rows. page_size (JSON ) number rows requested per chunk. recommended leave unspecified evidence page_size selected automatically bq_table_download() problematic. page_size = NULL bigrquery determines conservative, natural chunk size empirically. specify page_size, important chunk fits one page, .e. requested row limit low enough prevent API paginating based response size. start_index (JSON ) Starting row index (zero-based). max_connections (JSON ) Number maximum simultaneous connections BigQuery servers. quiet FALSE, displays progress bar; TRUE silent; NA picks based whether interactive context. bigint R type BigQuery's 64-bit integer types mapped . default \"integer\", returns R's integer type, results NA values /+/- 2147483647. \"integer64\" returns bit64::integer64, allows full range 64 bit integers. api API use? \"json\" API works ever bigrquery , slow can require fiddling page_size parameter. \"arrow\" API faster reliable, works also installed bigrquerystorage package. \"arrow\" API much faster, used automatically bigrquerystorage package installed. billing (Arrow ) Project bill; defaults project x, typically needs specified working public datasets. max_results Deprecated. Please use n_max instead.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_table_download.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download table data — bq_table_download","text":"data retrieval may generate list-columns data.frame print method can problems list-columns, method returns tibble. need data.frame, coerce results .data.frame().","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_table_download.html","id":"google-bigquery-api-documentation","dir":"Reference","previous_headings":"","what":"Google BigQuery API documentation","title":"Download table data — bq_table_download","text":"list","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_table_download.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download table data — bq_table_download","text":"","code":"if (FALSE) { # bq_testable() df <- bq_table_download(\"publicdata.samples.natality\", n_max = 35000, billing = bq_test_project()) }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_test_project.html","id":null,"dir":"Reference","previous_headings":"","what":"Project to use for testing bigrquery — bq_test_project","title":"Project to use for testing bigrquery — bq_test_project","text":"need set BIGQUERY_TEST_PROJECT (name project) BIGQUERY_TEST_BUCKET (name bucket) env vars order run bigrquery tests locally. recommend creating new project tests involve reading writing BigQuery Cloud Storage. BIGQUERY_TEST_PROJECT must billing enabled project. logged , via bq_auth(), user permission work BIGQUERY_TEST_PROJECT, run bq_test_init() perform setup.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_test_project.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Project to use for testing bigrquery — bq_test_project","text":"","code":"bq_test_project()  bq_test_init(name = \"basedata\")  bq_test_dataset(name = random_name(), location = \"US\")  bq_testable()  bq_authable()  gs_test_bucket()  gs_test_object(name = random_name())"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_test_project.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Project to use for testing bigrquery — bq_test_project","text":"name Dataset name - used testing.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_test_project.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Project to use for testing bigrquery — bq_test_project","text":"bq_test_project() returns name project suitable use testing. bq_test_dataset() creates temporary dataset whose lifetime tied lifetime object returns.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_test_project.html","id":"testing","dir":"Reference","previous_headings":"","what":"Testing","title":"Project to use for testing bigrquery — bq_test_project","text":"tests, bq_test_project() (hence bq_test_dataset()) automatically skip auth test project available.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_test_project.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Project to use for testing bigrquery — bq_test_project","text":"","code":"if (FALSE) { # bq_testable() ds <- bq_test_dataset() bq_mtcars <- bq_table_upload(bq_table(ds, \"mtcars\"), mtcars)  # dataset and table will be automatically deleted when ds is GC'd }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Produce configured token — bq_token","title":"Produce configured token — bq_token","text":"internal use programming around BigQuery API. Returns token pre-processed httr::config(). users need handle tokens \"hand\" , even need control, bq_auth() need. current token, bq_auth() called either load cache initiate OAuth2.0 flow. auth deactivated via bq_deauth(), bq_token() returns NULL.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Produce configured token — bq_token","text":"","code":"bq_token()"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_token.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Produce configured token — bq_token","text":"request object (S3 class provided httr).","code":""},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_token.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Produce configured token — bq_token","text":"","code":"if (FALSE) { # \\dontrun{ bq_token() } # }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Get info on current user — bq_user","title":"Get info on current user — bq_user","text":"Reveals email address user associated current token. token loaded yet, function initiate auth.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_user.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get info on current user — bq_user","text":"","code":"bq_user()"},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_user.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get info on current user — bq_user","text":"email address , token loaded, NULL.","code":""},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/reference/bq_user.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get info on current user — bq_user","text":"","code":"if (FALSE) { # \\dontrun{ bq_user() } # }"},{"path":"https://bigrquery.r-dbi.org/dev/reference/collect.tbl_BigQueryConnection.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect a BigQuery table — collect.tbl_BigQueryConnection","title":"Collect a BigQuery table — collect.tbl_BigQueryConnection","text":"collect method specialised BigQuery tables, generating SQL dplyr commands, calling bq_project_query() bq_dataset_query() run query, bq_table_download() download results. Thus arguments combination arguments dplyr::collect(), bq_project_query()/bq_dataset_query(), bq_table_download().","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/collect.tbl_BigQueryConnection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect a BigQuery table — collect.tbl_BigQueryConnection","text":"","code":"collect.tbl_BigQueryConnection(   x,   ...,   n = Inf,   api = c(\"json\", \"arrow\"),   page_size = NULL,   max_connections = 6L )"},{"path":"https://bigrquery.r-dbi.org/dev/reference/collect.tbl_BigQueryConnection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect a BigQuery table — collect.tbl_BigQueryConnection","text":"x data frame, data frame extension (e.g. tibble), lazy data frame (e.g. dbplyr dtplyr). See Methods, , details. ... arguments passed bq_project_query()/bq_project_query() n Maximum number results retrieve. default, Inf, retrieve rows. api API use? \"json\" API works ever bigrquery , slow can require fiddling page_size parameter. \"arrow\" API faster reliable, works also installed bigrquerystorage package. \"arrow\" API much faster, used automatically bigrquerystorage package installed. page_size (JSON ) number rows requested per chunk. recommended leave unspecified evidence page_size selected automatically bq_table_download() problematic. page_size = NULL bigrquery determines conservative, natural chunk size empirically. specify page_size, important chunk fits one page, .e. requested row limit low enough prevent API paginating based response size. max_connections (JSON ) Number maximum simultaneous connections BigQuery servers.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/src_bigquery.html","id":null,"dir":"Reference","previous_headings":"","what":"A BigQuery data source for dplyr. — src_bigquery","title":"A BigQuery data source for dplyr. — src_bigquery","text":"Create connection database DBI::dbConnect() use dplyr::tbl() connect tables within database. Generally, best provide fully qualified name table (.e. project.dataset.table) supply default dataset connection, can use just table name. (, however, prevent making joins across datasets.)","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/src_bigquery.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A BigQuery data source for dplyr. — src_bigquery","text":"","code":"src_bigquery(project, dataset, billing = project, max_pages = 10)"},{"path":"https://bigrquery.r-dbi.org/dev/reference/src_bigquery.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A BigQuery data source for dplyr. — src_bigquery","text":"project project id name dataset dataset name billing billing project, different project max_pages (IGNORED) maximum pages returned query","code":""},{"path":"https://bigrquery.r-dbi.org/dev/reference/src_bigquery.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A BigQuery data source for dplyr. — src_bigquery","text":"","code":"if (FALSE) { # \\dontrun{ library(dplyr)  # To run this example, replace billing with the id of one of your projects # set up for billing con <- DBI::dbConnect(bigquery(), project = bq_test_project())  shakespeare <- con %>% tbl(I(\"publicdata.samples.shakespeare\")) shakespeare shakespeare %>%   group_by(word) %>%   summarise(n = sum(word_count, na.rm = TRUE)) %>%   arrange(desc(n)) } # }"},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-development-version","dir":"Changelog","previous_headings":"","what":"bigrquery (development version)","title":"bigrquery (development version)","text":"Various R CMD check fixes bigrquerystorage package installed, bq_table_download() (hence collect(), dbGetQuery() dbFetch()) use . drastically improve speed downloading large datasets. big thanks @meztez creating bigrquerystorage package! bq_perform_upload() function now allows users choose transmission format (JSON PARQUET) data sent BigQuery (@apalacio9502, #608). bigrquery now requires R 4.1, line version support principles.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-151","dir":"Changelog","previous_headings":"","what":"bigrquery 1.5.1","title":"bigrquery 1.5.1","text":"CRAN release: 2024-03-14 Forward compatibility upcoming dbplyr release (#601).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-150","dir":"Changelog","previous_headings":"","what":"bigrquery 1.5.0","title":"bigrquery 1.5.0","text":"CRAN release: 2024-01-22","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"major-changes-1-5-0","dir":"Changelog","previous_headings":"","what":"Major changes","title":"bigrquery 1.5.0","text":"bigrquery now MIT licensed (#453). Deprecated functions (.e. starting bq_) removed (#551). superseded long time formally deprecated bigrquery 1.3.0 (2020). bq_table_download() now returns unknown fields character vectors. means BIGNUMERIC (#435) JSON (#544) data downloaded R process wish. now parses dates using clock package. leads considerable performance improvement (#430) ensures dates prior 1970-01-01 parsed correctly (#285).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"significant-dbi-improvements-1-5-0","dir":"Changelog","previous_headings":"","what":"Significant DBI improvements","title":"bigrquery 1.5.0","text":"bigquery datasets tables now appear connection pane using dbConnect (@meztez, #431). dbAppendTable() (#539), dbCreateTable() (#483), dbExecute (#502) now supported. dbGetQuery()/dbSendQuery() gains support parameterised queries via params argument (@byapparov, #444). dbReadTable(), dbWriteTable(), dbExistsTable(), dbRemoveTable(), dbListFields() now work DBI::Id() (#537).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"significant-dbplyr-improvements-1-5-0","dir":"Changelog","previous_headings":"","what":"Significant dbplyr improvements","title":"bigrquery 1.5.0","text":"bigrquery now uses 2nd edition dbplyr interface (#508) compatible dbplyr 2.4.0 (#550). Joins now work correctly across bigrquery connections (#433). grepl(pattern, x) now correctly translated REGEXP_CONTAINS(x, pattern) (#416). median() gets translation works summarise() clear error use mutate() (#419). tbl() now works views (#519), including views found INFORMATION_SCHEMA schema (#468). tbl(con, sql(\"...\")) now works robustly (#540), fixing “URL using bad/illegal format missing URL” error. runif(n()) gains translation slice_sample() can work (@mgirlich, #448).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"minor-improvements-and-bug-fixes-1-5-0","dir":"Changelog","previous_headings":"","what":"Minor improvements and bug fixes","title":"bigrquery 1.5.0","text":"Google API URLs aligned Google Cloud Discovery docs. enables support Private Restricted Google APIs configurations (@husseyd, #541) Functions generally try better job telling ’ve supplied wrong type input. Additionally, supply SQL() query, longer get weird warning (#498). bq_job_wait() receives 503 response, now waits 2 seconds tries (#535). dbFetch() now respects quiet setting connection (#463). dbGetRowCount() dbHasComplete() now return correct values try fetch rows actually exist (#501). New dbQuoteLiteral() method logicals reverts breaking change introduced DBI 1.1.2 (@meztez, #478). dbWriteTable() now correct uses billing value set connection (#486).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-142","dir":"Changelog","previous_headings":"","what":"bigrquery 1.4.2","title":"bigrquery 1.4.2","text":"CRAN release: 2023-04-20 Sync current release gargle (1.4.0). Recently gargle introduced changes around OAuth bigrquery syncing : bq_oauth_client() new function replace now-deprecated bq_oauth_app(). new client argument bq_auth_configure() replaces now-deprecated client argument. documentation bq_auth_configure() emphasizes preferred way “bring OAuth client” providing JSON downloaded Google Developers Console. op_table.lazy_select_query() now returns string instead list, fixes error seen printing using functions like head() dplyr::glimpse() (@clente, #509).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-141","dir":"Changelog","previous_headings":"","what":"bigrquery 1.4.1","title":"bigrquery 1.4.1","text":"CRAN release: 2022-10-27 Fix R CMD check R-devel (#511) bigrquery now compatible dbplyr 2.2.0 (@mgirlich, #495). brio new Imports, replacing use Suggested package readr, bq_table_download() (@AdeelK93, #462).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-140","dir":"Changelog","previous_headings":"","what":"bigrquery 1.4.0","title":"bigrquery 1.4.0","text":"CRAN release: 2021-08-05 bq_table_download() heavily refactored (#412): now return requested results, full, situations. However, “row shortage”, throws error instead silently returning incomplete results. max_results argument deprecated favor n_max, reflects actually number consistent n_max argument elsewhere, e.g., readr::read_csv(). default value page_size longer fixed , instead, determined empirically. Users strongly recommended let bigrquery select page_size automatically, unless ’s specific reason otherwise. BigQueryResult object gains billing slot (@meztez, #423). collect.tbl_BigQueryConnection() honours bigint field found connection object created DBI::dbConnect() passes bigint along bq_table_download(). improves support 64-bit integers reading BigQuery tables dplyr syntax (@zoews, #439, #437).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-132","dir":"Changelog","previous_headings":"","what":"bigrquery 1.3.2","title":"bigrquery 1.3.2","text":"CRAN release: 2020-10-05 BigQuery BYTES GEOGRAPHY column types now supported via blob wk packages, respectively (@paleolimbot, #354, #388). used dbplyr >= 2.0.0, ambiguous variables joins get suffixes _x _y (instead .x .y don’t work BigQuery) (#403). bq_table_download() works large row counts (@gjuggler, #395). Google’s API stopped accepting startIndex parameters scientific formatting, happening large values (>1e5) default. New bq_perform_query_dry_run() retrieve estimated cost performing query (@Ka2wei, #316).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-131","dir":"Changelog","previous_headings":"","what":"bigrquery 1.3.1","title":"bigrquery 1.3.1","text":"CRAN release: 2020-05-15 Now requires gargle 0.5.0","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-130","dir":"Changelog","previous_headings":"","what":"bigrquery 1.3.0","title":"bigrquery 1.3.0","text":"CRAN release: 2020-05-08 Old functions (starting bq_) deprecated (@byapparov, #335) bq_perform_*() fails, now see errors, just first (#355). bq_perform_query() can now execute parameterised query parameters ARRAY type (@byapparov, #303). Vectors length > 1 automatically converted ARRAY type, use bq_param_array() explicit. bq_perform_upload() works (#361). seems like generated JSON always incorrect, Google’s type checking recently become strict enough detect problem. dbExecute() better supported. longer fails spurious error DDL queries, returns number affected rows DML queries (#375). dbSendQuery() (hence dbGetQuery()) collect() passes ... bq_perform_query(). collect() gains page_size max_connection arguments passed bq_table_download() (#374). copy_to() now works BigQuery (although doesn’t support temporary tables application somewhat limited) (#337). str_detect() now correctly translated REGEXP_CONTAINS (@jimmyg3g, #369). Error messages include hints common problems (@deflaux, #353).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-120","dir":"Changelog","previous_headings":"","what":"bigrquery 1.2.0","title":"bigrquery 1.2.0","text":"CRAN release: 2019-07-02","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"auth-from-gargle-1-2-0","dir":"Changelog","previous_headings":"","what":"Auth from gargle","title":"bigrquery 1.2.0","text":"bigrquery’s auth functionality now comes gargle package, provides R infrastructure work Google APIs, general. transition underway several packages, googledrive. make user interfaces consistent makes two new token flows available bigrquery: Application Default Credentials Service account tokens metadata server available VMs running GCE learn : Help bq_auth() users need gargle gets tokens Non-interactive auth get API credentials","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"changes-that-a-user-will-notice-1-2-0","dir":"Changelog","previous_headings":"Auth from gargle","what":"Changes that a user will notice","title":"bigrquery 1.2.0","text":"Temporary files now deleted table download. (@meztez, #343) OAuth2 tokens now cached user level, default, instead .httr-oauth current project. default OAuth app also changed. means need re-authorize bigrquery (.e. get new token). may want delete vestigial .httr-oauth files lying around bigrquery projects. OAuth2 token key-value store now incorporates associated Google user indexing, makes easier switch Google identities. bq_user() new function reveals email user associated current token. previously used set_service_token() use service account token, still works. ’ll get deprecation warning. Switch bq_auth(path = \"/path///service-account.json\"). Several functions similarly soft-deprecated.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"dependency-changes-1-2-0","dir":"Changelog","previous_headings":"","what":"Dependency changes","title":"bigrquery 1.2.0","text":"R 3.1 longer explicitly supported tested. general practice support current release (3.6), devel, 4 previous versions R (3.5, 3.4, 3.3, 3.2). gargle rlang newly Imported.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-111","dir":"Changelog","previous_headings":"","what":"bigrquery 1.1.1","title":"bigrquery 1.1.1","text":"CRAN release: 2019-05-16 Fix test failure dbplyr 1.4.0. bq_field() can now pass description parameter applied bq_table_create() call (@byapparov, #272). bq_table_patch() - allows patch table (@byapparov, #253) new schema.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-110","dir":"Changelog","previous_headings":"","what":"bigrquery 1.1.0","title":"bigrquery 1.1.0","text":"CRAN release: 2019-02-05","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"improved-type-support-1-1-0","dir":"Changelog","previous_headings":"","what":"Improved type support","title":"bigrquery 1.1.0","text":"bq_table_download() DBI::dbConnect method now bigint argument governs BigQuery integer columns imported R. , default bigint = \"integer\". can set bigint = \"integer64\" import BigQuery integer columns bit64::integer64 columns R allows values outside range integer (-2147483647 2147483647) (@rasmusab, #94). bq_table_download() now treats NUMERIC columns FLOAT columns (@paulsendavidjay, #282). bq_table_upload() works POSIXct/POSIXct variables (#251)","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"sql-translation-1-1-0","dir":"Changelog","previous_headings":"","what":"SQL translation","title":"bigrquery 1.1.0","text":".character() now translated SAFE_CAST(x STRING) (#268). median() now translates APPROX_QUANTILES(x, 2)[SAFE_ORDINAL(2)] (@valentinumbach, #267).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"minor-bug-fixes-and-improvements-1-1-0","dir":"Changelog","previous_headings":"","what":"Minor bug fixes and improvements","title":"bigrquery 1.1.0","text":"Jobs now print ids running (#252) bq_job() tracks location bigrquery now works painlessly non-US/EU locations (#274). bq_perform_upload() autodetect schema table already exist. bq_table_download() correctly computes page ranges max_results start_index supplied (#248) Unparseable date times return NA (#285)","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"bigrquery-100","dir":"Changelog","previous_headings":"","what":"bigrquery 1.0.0","title":"bigrquery 1.0.0","text":"CRAN release: 2018-04-24","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"improved-downloads-1-0-0","dir":"Changelog","previous_headings":"","what":"Improved downloads","title":"bigrquery 1.0.0","text":"system downloading data BigQuery R rewritten ground give considerable improvements performance flexibility. two steps, downloading parsing, now happen sequence, rather interleaved. means ’ll now see two progress bars: one downloading JSON BigQuery one parsing JSON data frame. Downloads now occur parallel, using 6 simultaneous connections default. parsing code rewritten C++. well considerably improving performance, also adds support nested (record/struct) repeated (array) columns (#145). columns yield list-columns following forms: Repeated values become list-columns containing vectors. Nested values become list-columns containing named lists. Repeated nested values become list-columns containing data frames. Results now returned tibbles, data frames, base print method handle list columns well. can now download first million rows publicdata.samples.natality minute. data frame 170 MB BigQuery 140 MB R; minute download much data seems reasonable . bottleneck loading BigQuery data now parsing BigQuery’s json format. don’t see obvious way make faster ’m already using fastest C++ json parser, RapidJson. still slow (.e. ’re downloading GBs data), see ?bq_table_download alternative approach.","code":""},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"dplyr-1-0-0","dir":"Changelog","previous_headings":"New features","what":"dplyr","title":"bigrquery 1.0.0","text":"dplyr::compute() now works (@realAkhmed, #52). tbl() now accepts fully (partially) qualified table names, like “publicdata.samples.shakespeare” “samples.shakespeare”. makes possible join tables across datasets (#219).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"dbi-1-0-0","dir":"Changelog","previous_headings":"New features","what":"DBI","title":"bigrquery 1.0.0","text":"dbConnect() now defaults standard SQL, rather legacy SQL. Use use_legacy_sql = TRUE need previous behaviour (#147). dbConnect() now allows dataset omitted; natural want use tables multiple datasets. dbWriteTable() dbReadTable() now accept fully (partially) qualified table names. dbi_driver() deprecated; please use bigquery() instead.","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"low-level-api-1-0-0","dir":"Changelog","previous_headings":"New features","what":"Low-level API","title":"bigrquery 1.0.0","text":"low-level API completely overhauled make easier use. primary motivation make bigrquery development enjoyable , also helpful need go outside features provided higher-level DBI dplyr interfaces. old API soft-deprecated - continue work, development occur (including bug fixes). formally deprecated next version, removed version . Consistent naming scheme: API functions now form bq_object_verb(), e.g. bq_table_create(), bq_dataset_delete(). S3 classes: bq_table(), bq_dataset(), bq_job(), bq_field() bq_fields() constructor functions create S3 objects corresponding important BigQuery objects (#150). paired as_ coercion functions used throughout new API. Easier local testing: New bq_test_project() bq_test_dataset() make easier run bigrquery tests locally. run tests , need create BigQuery project, follow instructions ?bq_test_project. efficient data transfer: new API makes extensive use fields query parameter, ensuring functions download data actually use (#153). Tighter GCS connection: New bq_table_load() loads data Google Cloud Storage URI, pairing bq_table_save() saves data GCS URI (#155).","code":""},{"path":[]},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"dplyr-1-0-0-1","dir":"Changelog","previous_headings":"Bug fixes and minor improvements","what":"dplyr","title":"bigrquery 1.0.0","text":"dplyr interface can work literal SQL (#218). Improved SQL translation pmax(), pmin(), sd(), (), () (#176, #179, @jarodmeng). paste0(), cor() cov() (@edgararuiz). development version dbplyr installed, print()ing BigQuery table perform unneeded query, instead download directly table (#226).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"low-level-1-0-0","dir":"Changelog","previous_headings":"Bug fixes and minor improvements","what":"Low-level","title":"bigrquery 1.0.0","text":"Request error messages now contain “reason”, can contain useful information debugging (#209). bq_dataset_query() bq_project_query() can now supply query parameters (#191). bq_table_create() can now specify fields (#204). bq_perform_query() longer fails empty results (@byapparov, #206).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"version-041","dir":"Changelog","previous_headings":"","what":"Version 0.4.1","title":"Version 0.4.1","text":"CRAN release: 2017-06-26 Fix SQL translation omissions discovered dbplyr 1.1.0","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"version-040","dir":"Changelog","previous_headings":"","what":"Version 0.4.0","title":"Version 0.4.0","text":"CRAN release: 2017-06-23","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"new-features-0-4-0","dir":"Changelog","previous_headings":"","what":"New features","title":"Version 0.4.0","text":"dplyr support updated require dplyr 0.7.0 use dbplyr. means can now naturally work directly DBI connections. dplyr now also uses modern BigQuery SQL supports broader set translations. Along way ’ve also fixed SQL generation bugs (#48). DBI driver gets new name: bigquery(). New insert_extract_job() make possible extract data save google storage (@realAkhmed, #119). New insert_table() allows insert empty tables dataset. POST requests (inserts, updates, copies query_exec) now take .... allows add arbitrary additional data request body making possible use parts BigQuery API otherwise exposed (#149). snake_case argument names automatically converted camelCase can stick consistently snake case R code. Full support DATE, TIME, DATETIME types (#128).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"big-fixes-and-minor-improvements-0-4-0","dir":"Changelog","previous_headings":"","what":"Big fixes and minor improvements","title":"Version 0.4.0","text":"bigrquery requests now custom user agent specifies versions bigrquery httr used (#151). dbConnect() gains new use_legacy_sql, page_size, quiet arguments passed onto query_exec(). allow control query options connection level. insert_upload_job() now sends data newline-delimited JSON instead csv (#97). considerably faster avoids character encoding issues (#45). POSIXlt columns now also correctly coerced TIMESTAMPS (#98). insert_query_job() query_exec() gain new arguments: quiet = TRUE suppress progress bars needed. use_legacy_sql = FALSE option allows opt-legacy SQL system (#124, @backlin) list_tables() (#108) list_datasets() (#141) now paginated. default retrieve 50 items per page, iterate get everything. list_tabledata() query_exec() now give nicer progress bar, including estimated time remaining (#100). query_exec() considerably faster profiling revealed ~40% time taken single line inside function helps parse BigQuery’s json R data frame. replaced slow R code faster C function. set_oauth2.0_cred() allows user supply Google OAuth application setting credentials (#130, @jarodmeng) wait_for() uses now reports query total bytes billed, accurate takes account caching factors. list_tabledata returns empty table max_pages=0 (#184, @ras44 @byapparov)","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"version-030","dir":"Changelog","previous_headings":"","what":"Version 0.3.0","title":"Version 0.3.0","text":"CRAN release: 2016-06-28 New set_service_token() allows use OAuth service token instead interactive authentication.^ correctly translated pow() (#110). Provide full DBI compliant interface (@krlmlr). Backend now translates iflese() (@realAkhmed, #53).","code":""},{"path":"https://bigrquery.r-dbi.org/dev/news/index.html","id":"version-020","dir":"Changelog","previous_headings":"","what":"Version 0.2.0.","title":"Version 0.2.0.","text":"CRAN release: 2016-03-03 Compatible latest httr. Computation SQL data type corresponds given R object now robust unknown classes. (#95, @krlmlr) data frame full schema information returned zero-row results. (#88, @krlmlr) New exists_table(). (#91, @krlmlr) New arguments create_disposition write_disposition insert_upload_job(). (#92, @krlmlr) Renamed option bigquery.quiet bigrquery.quiet. (#89, @krlmlr) New format_dataset() format_table(). (#81, @krlmlr) New list_tabledata_iter() allows fetching table chunks varying size. (#77, #87, @krlmlr) Add support API keys via BIGRQUERY_API_KEY environment variable. (#49)","code":""}]
